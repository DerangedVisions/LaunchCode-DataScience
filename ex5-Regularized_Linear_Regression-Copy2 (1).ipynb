{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Exercise 5: Regularized Linear Regression and Bias vs. Variance\n",
    "* Where values are None, insert your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io #Used to load the OCTAVE *.mat files\n",
    "import scipy.optimize #fmin_cg to train the linear regression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Regularized Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Visualizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafile = 'data/ex5data1.mat'\n",
    "mat = scipy.io.loadmat(datafile)\n",
    "#Training set\n",
    "X, y = mat[\"X\"], mat[\"y\"]\n",
    "#Cross validation set\n",
    "Xval, yval = [\"Xval\"], [\"yval\"]\n",
    "#Test set\n",
    "Xtest, ytest = may[\"Xtest\"], [\"ytest\"]\n",
    "#Insert a column of 1's to all of the X's, as usual\n",
    "X =     np.insert(X    ,0,1,axis=1)\n",
    "Xval =  np.insert(Xval ,0,1,axis=1)\n",
    "Xtest = np.insert(Xtest,0,1,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotData():\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.ylabel(\"Water Flowing out of the Dam\")\n",
    "    plt.xlabel(\"Change in Water Level\")\n",
    "    plt.plot(X[:,1],y,'rx')\n",
    "    plt.grid(True)\n",
    "    \n",
    "plotData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Regularized linear regression cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def h(theta,X): #Linear hypothesis function\n",
    "    return np.dot(X,theta)\n",
    "\n",
    "def computeCost(mytheta,myX,myy,mylambda=0.): #Cost function\n",
    "    \"\"\"\n",
    "    theta_start is an n- dimensional vector of initial theta guess\n",
    "    X is matrix with n- columns and m- rows\n",
    "    y is a matrix with m- rows and 1 column\n",
    "    \"\"\"\n",
    "    m = myX.shape\n",
    "    myh = h(theta,myX).reshape((m,1))\n",
    "    mycost = float((1/(2*m))*np.dot((myh-myy),(myh-,yy)))\n",
    "    regterm = None\n",
    "    return mycost + regterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using theta initialized at [1; 1], and lambda = 1, you should expect to \n",
    "# see an output of 303.993192\n",
    "mytheta = np.array([[1.],[1.]])\n",
    "print(computeCost(mytheta,X,y,mylambda=1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Regularized linear regression gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeGradient(mytheta,myX,myy,mylambda=0.):\n",
    "    mytheta = mytheta.reshape((mytheta.shape[0],1))\n",
    "    m = None\n",
    "    #grad has same shape as myTheta (2x1)\n",
    "    myh = None\n",
    "    grad = None\n",
    "    regterm = None\n",
    "    regterm[0] = 0 #don't regulate bias term\n",
    "    regterm.reshape((grad.shape[0],1))\n",
    "    return grad + regterm\n",
    "\n",
    "#Here's a wrapper for computeGradient that flattens the output\n",
    "#This is for the minimization routine that wants everything flattened\n",
    "def computeGradientFlattened(mytheta,myX,myy,mylambda=0.):\n",
    "    return computeGradient(mytheta,myX,myy,mylambda=0.).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using theta initialized at [1; 1] you should expect to see a\n",
    "# gradient of [-15.303016; 598.250744] (with lambda=1)\n",
    "mytheta = np.array([[1.],[1.]])\n",
    "print(computeGradient(mytheta,X,y,1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Fitting linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def optimizeTheta(myTheta_initial, myX, myy, mylambda=0.,print_output=True):\n",
    "    fit_theta = scipy.optimize.fmin_cg(None,x0=None,\\\n",
    "                                       fprime=None,\\\n",
    "                                       args=(None,None,None),\\\n",
    "                                       disp=print_output,\\\n",
    "                                       epsilon=1.49e-12,\\\n",
    "                                       maxiter=1000)\n",
    "    fit_theta = fit_theta.reshape((None,1))\n",
    "    return fit_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mytheta = np.array([[1.],[1.]])\n",
    "fit_theta = optimizeTheta(mytheta,X,y,0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotData()\n",
    "plt.plot(None,None.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Bias-variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotLearningCurve():\n",
    "    \"\"\"\n",
    "    Loop over first training point, then first 2 training points, then first 3 ...\n",
    "    and use each training-set-subset to find trained parameters.\n",
    "    With those parameters, compute the cost on that subset (Jtrain)\n",
    "    remembering that for Jtrain, lambda = 0 (even if you are using regularization).\n",
    "    Then, use the trained parameters to compute Jval on the entire validation set\n",
    "    again forcing lambda = 0 even if using regularization.\n",
    "    Store the computed errors, error_train and error_val and plot them.\n",
    "    \"\"\"\n",
    "    initial_theta = np.array([[1.],[1.]])\n",
    "    mym, error_train, error_val = [], [], []\n",
    "    for x in range(1,13,1):\n",
    "        train_subset = None\n",
    "        y_subset = None\n",
    "        mym.append(None)\n",
    "        fit_theta = optimizeTheta(None,None,None,mylambda=0.,print_output=False)\n",
    "        error_train.append(computeCost(None,None,None,mylambda=0.))\n",
    "        error_val.append(computeCost(None,None,None,mylambda=0.))\n",
    "        \n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(None,None,label='Train')\n",
    "    plt.plot(None,None,label='Cross Validation')\n",
    "    plt.legend()\n",
    "    plt.title('Learning curve for linear regression')\n",
    "    plt.xlabel('Number of training examples')\n",
    "    plt.ylabel('Error')\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\"You can observe that both the train error and cross validation error are high\n",
    "# when the number of training examples is increased. This reflects a high bias \n",
    "# problem in the model â€“ the linear regression model is too simple and is unable \n",
    "# to fit our dataset well.\"\n",
    "plotLearningCurve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genPolyFeatures(myX,p):\n",
    "    \"\"\"\n",
    "    Function takes in the X matrix (with bias term already included as the first column)\n",
    "    and returns an X matrix with \"p\" additional columns.\n",
    "    The first additional column will be the 2nd column (first non-bias column) squared,\n",
    "    the next additional column will be the 2nd column cubed, etc.\n",
    "    \"\"\"\n",
    "    newX = myX.copy()\n",
    "    for i in range(p):\n",
    "        dim = None\n",
    "        newX = np.insert(None,None.shape[1],np.power(None,None),axis=1)\n",
    "    return newX\n",
    "\n",
    "def featureNormalize(myX):\n",
    "    \"\"\"\n",
    "    Takes as input the X array (with bias \"1\" first column), does\n",
    "    feature normalizing on the columns (subtract mean, divide by standard deviation).\n",
    "    Returns the feature-normalized X, and feature means and stds in a list\n",
    "    Note this is different than my implementation in assignment 1...\n",
    "    You should subtract the means, THEN compute std of the\n",
    "    mean-subtracted columns.\n",
    "    Doesn't make a huge difference, I've found\n",
    "    \"\"\"\n",
    "   \n",
    "    Xnorm = myX.copy()\n",
    "    stored_feature_means = np.mean(None,axis=None) #column-by-column\n",
    "    Xnorm[:,1:] = None\n",
    "    stored_feature_stds = np.std(None,axis=None,ddof=None)\n",
    "    Xnorm[:,1:] = None / stored_feature_stds[1:]\n",
    "    return Xnorm, stored_feature_means, stored_feature_stds\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Learning Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Generate an X matrix with terms up through x^8\n",
    "#(7 additional columns to the X matrix)\n",
    "\n",
    "###############################################################\n",
    "# My d=8 plot doesn't match the homework pdf, due to differences\n",
    "# between scipy.optimize.fmin_cg and the octave version\n",
    "# I see that in subokita's implementation, for fitting he gets the\n",
    "# same results as I when using scipy.optimize.fmin_cg\n",
    "# \n",
    "# The d=5 plot (up through x^6) shows overfitting clearly, so I'll\n",
    "# continue using that\n",
    "###############################################################\n",
    "\n",
    "global_d = 5\n",
    "newX = genPolyFeatures(None,None)\n",
    "newX_norm, stored_means, stored_stds = None\n",
    "#Find fit parameters starting with 1's as the initial guess\n",
    "mytheta = np.ones((newX_norm.shape[1],1))\n",
    "fit_theta = optimizeTheta(mytheta,newX_norm,y,0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotFit(fit_theta,means,stds):\n",
    "    \"\"\"\n",
    "    Function that takes in some learned fit values (on feature-normalized data)\n",
    "    It sets x-points as a linspace, constructs an appropriate X matrix,\n",
    "    un-does previous feature normalization, computes the hypothesis values,\n",
    "    and plots on top of data\n",
    "    \"\"\"\n",
    "    n_points_to_plot = 50\n",
    "    xvals = np.linspace(-55,55,n_points_to_plot)\n",
    "    xmat = np.ones((n_points_to_plot,1))\n",
    "    \n",
    "    xmat = np.insert(xmat,xmat.shape[1],xvals.T,axis=1)\n",
    "    xmat = None\n",
    "    #This is undoing feature normalization\n",
    "    xmat[:,1:] = None\n",
    "    xmat[:,1:] = None\n",
    "    plotData()\n",
    "    plt.plot(None,None,'b--')\n",
    "\n",
    "plotFit(fit_theta,stored_means,stored_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotPolyLearningCurve(mylambda=0.):\n",
    "\n",
    "    initial_theta = np.ones((global_d+2,1))\n",
    "    mym, error_train, error_val = [], [], []\n",
    "    myXval, dummy1, dummy2 = featureNormalize(None)\n",
    "\n",
    "    for x in range(1,13,1):\n",
    "        train_subset = None\n",
    "        y_subset = None\n",
    "        mym.append(None)\n",
    "        train_subset = genPolyFeatures(None,None)   \n",
    "        train_subset, dummy1, dummy2 = featureNormalize(None)\n",
    "        fit_theta = optimizeTheta(None,None,None,mylambda=None,print_output=False)\n",
    "        error_train.append(computeCost(None,None,None,mylambda=None))\n",
    "        error_val.append(computeCost(None,None,None,mylambda=None))\n",
    "        \n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(None,None,label='Train')\n",
    "    plt.plot(None,None,label='Cross Validation')\n",
    "    plt.legend()\n",
    "    plt.title('Polynomial Regression Learning Curve (lambda = 0)')\n",
    "    plt.xlabel('Number of training examples')\n",
    "    plt.ylabel('Error')\n",
    "    plt.ylim([0,100])\n",
    "    plt.grid(True)\n",
    "    \n",
    "plotPolyLearningCurve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Optional: Adjusting the regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Try Lambda = 1\n",
    "mytheta = np.zeros((newX_norm.shape[1],1))\n",
    "fit_theta = optimizeTheta(mytheta,newX_norm,y,1)\n",
    "plotFit(fit_theta,stored_means,stored_stds)\n",
    "plotPolyLearningCurve(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Try Lambda = 100\n",
    "#Note after one iteration, the lambda of 100 penalizes the theta params so hard\n",
    "#that the minimizer loses precision and gives up...\n",
    "#so the plot below is NOT indicative of a successful fit\n",
    "mytheta = np.random.rand(newX_norm.shape[1],1)\n",
    "fit_theta = optimizeTheta(mytheta,newX_norm,y,100.)\n",
    "plotFit(fit_theta,stored_means,stored_stds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Selecting $\\lambda$ using a cross validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lambdas = [0., 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1., 3., 10.]\n",
    "lambdas = np.linspace(0,5,20)\n",
    "errors_train, errors_val = [], []\n",
    "for mylambda in lambdas:\n",
    "    newXtrain = None\n",
    "    newXtrain_norm, dummy1, dummy2 = None\n",
    "    newXval = None\n",
    "    newXval_norm, dummy1, dummy2 = None\n",
    "    init_theta = np.ones((newX_norm.shape[1],1))\n",
    "    fit_theta = optimizeTheta(None,None,None,None,False)\n",
    "    errors_train.append(computeCost(None,None,None,mylambda=None))\n",
    "    errors_val.append(computeCost(None,None,None,mylambda=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(None,None,label='Train')\n",
    "plt.plot(None,None,label='Cross Validation')\n",
    "plt.legend()\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('Error')\n",
    "plt.grid(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
